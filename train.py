import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œë°”
import os  

# PYTORCH_ENABLE_MPS_FALLBACK í™˜ê²½ ë³€ìˆ˜ë¥¼ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ angle()ì²˜ëŸ¼ ì§€ì› ì•ˆ ë˜ëŠ” ì—°ì‚°ì€ CPUë¡œ ìë™ ì „í™˜ë©ë‹ˆë‹¤.
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

from spectrogram import Spectrogram
from train_dataset import NoiseRemovalDataset
from unet import UNetDenoise

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì‹¤í—˜ ì¡°ê±´) ---
BATCH_SIZE = 8  # í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ìˆ˜ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì¤„ì´ì„¸ìš”: 8 or 4)
LEARNING_RATE = 1e-4  # í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¼)
EPOCHS = 20  # ì „ì²´ ë°ì´í„°ì…‹ ë°˜ë³µ íšŸìˆ˜
NUM_WORKERS = 0  # ë°ì´í„° ë¡œë”©ì— ì‚¬ìš©í•  CPU ì½”ì–´ ìˆ˜
RESUME_FROM_EPOCH = 7  # ì´ì–´ì„œ í•™ìŠµí•  ë•Œ ìˆ˜ì •í•˜ëŠ” ë³€ìˆ˜ : 0ì´ë©´ ì²˜ìŒë¶€í„° ì‹œì‘, 3ì´ë©´ epoch 3ë²ˆ íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ 4ë²ˆë¶€í„° ì‹œì‘

# ê²½ë¡œ ì„¤ì • (ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
CLEAN_DIR = "./data/LibriSpeech/train-clean-100/"
NOISE_DIR = "./data/noise_datasets/audio/"
SAVE_DIR = "./checkpoints/"  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ


def get_device():
    """ì¥ì¹˜ ìë™ ê°ì§€: Mac(MPS), NVIDIA(CUDA), CPU ìˆœì„œ"""
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")


def train():
    # 1. ì´ˆê¸° ì„¤ì •
    local_start_epoch = RESUME_FROM_EPOCH
    device = get_device()
    print(f"ğŸš€ í•™ìŠµ ì¥ì¹˜ ì„¤ì •: {device}")

    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    # 2. ë°ì´í„°ì…‹ ë° ë¡œë” ì¤€ë¹„
    # (ì‹œë£Œë¥¼ ì¥ë¹„ì— ë„£ê¸° ì¢‹ê²Œ í¬ì¥í•˜ëŠ” ê³¼ì •)
    print("ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
    train_dataset = NoiseRemovalDataset(CLEAN_DIR, NOISE_DIR)
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,  # ë°ì´í„°ë¥¼ ì˜ ì„ì–´ì•¼ í•™ìŠµì´ ì˜ ë¨ (Ergodicity)
        num_workers=NUM_WORKERS,
        pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
    )
    print(f"âœ… ì´ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}")

    spec_converter = Spectrogram().to(device)

    # 3. ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì¤€ë¹„
    model = UNetDenoise().to(device)

    # ì´ì–´ì„œ í•™ìŠµí•˜ê¸° ë¡œì§
    if local_start_epoch > 0:
        checkpoint_path = os.path.join(SAVE_DIR, f"unet_epoch_{RESUME_FROM_EPOCH}.pth")

        if os.path.isfile(checkpoint_path):
            print(
                f"ğŸ”„ {RESUME_FROM_EPOCH}ë²ˆ ì—í­ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: {checkpoint_path}"
            )
            # ì €ì¥ëœ ê°€ì¤‘ì¹˜(Weight)ë¥¼ ëª¨ë¸ì— ë®ì–´ì”Œì›€
            model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        else:
            print(f"âš ï¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            print("ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
            local_start_epoch = 0

    # Loss Function: MSE (Mean Squared Error)
    # í”½ì…€ê°’(dB)ì˜ ì°¨ì´ë¥¼ ì œê³±í•´ì„œ í‰ê·  ëƒ„ -> ì´ê±¸ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œ
    criterion = nn.MSELoss()

    # Optimizer: Adam
    # ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì„ ë˜‘ë˜‘í•˜ê²Œ ìˆ˜í–‰í•˜ëŠ” ë„êµ¬
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 4. í•™ìŠµ ë£¨í”„ (Training Loop)
    print("\nğŸ”¥ í•™ìŠµ ì‹œì‘!")
    model.train()  # í•™ìŠµ ëª¨ë“œ ì „í™˜ (BN, Dropout ë“± í™œì„±í™”)

    for epoch in range(RESUME_FROM_EPOCH, EPOCHS):
        running_loss = 0.0
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  ë°” í‘œì‹œ
        loop = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{EPOCHS}]")

        for batch_idx, (mixed_wave, clean_wave) in enumerate(loop):
            # mixed_spec: ì…ë ¥ (ë…¸ì´ì¦ˆ ë‚Œ)
            # clean_spec: ì •ë‹µ (ê¹¨ë—í•¨)
            # _: ìœ„ìƒ ì •ë³´ëŠ” í•™ìŠµ ë•ŒëŠ” í•„ìš” ì—†ìŒ (ë³µì› ë•Œë§Œ ì‚¬ìš©)

            # ë°ì´í„°ë¥¼ GPU(MPS)ë¡œ ì´ë™
            mixed_wave = mixed_wave.to(device)
            clean_wave = clean_wave.to(device)

            with torch.no_grad():
                mixed_spec,_ = spec_converter.to_spec(mixed_wave)
                clean_spec,_ = spec_converter.to_spec(clean_wave)

            # --- Forward Pass (ì˜ˆì¸¡) ---
            predictions = model(mixed_spec)

            # --- Compute Loss (ì˜¤ì°¨ ê³„ì‚°) ---
            loss = criterion(predictions, clean_spec)

            # --- Backward Pass (ì—­ì „íŒŒ & ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸) ---
            optimizer.zero_grad()  # ì´ì „ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
            loss.backward()  # ê¸°ìš¸ê¸° ê³„ì‚° (Gradient Calculation)
            optimizer.step()  # ê°€ì¤‘ì¹˜ ìˆ˜ì • (Parameter Update)

            # --- Logging ---
            running_loss += loss.item()
            loop.set_postfix(loss=loss.item())  # ì§„í–‰ë°”ì— í˜„ì¬ ì†ì‹¤ í‘œì‹œ

        # ì—í­ ì¢…ë£Œ í›„ í‰ê·  ì†ì‹¤ ì¶œë ¥
        avg_loss = running_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{EPOCHS}] Average Loss: {avg_loss:.6f}")

        # ëª¨ë¸ ì €ì¥ (Checkpoint)
        # ë‚˜ì¤‘ì— í•™ìŠµ ëŠê²¨ë„ ì—¬ê¸°ì„œë¶€í„° ë‹¤ì‹œ í•˜ê±°ë‚˜, ê²°ê³¼ë¬¼ í™•ì¸ìš©
        torch.save(
            model.state_dict(), os.path.join(SAVE_DIR, f"unet_epoch_{epoch+1}.pth")
        )

    print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ëª¨ë“  ëª¨ë¸ì´ checkpoints í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    train()
